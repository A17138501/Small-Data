{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eabfa717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done.\n",
      "/Users/yioha_/Desktop/Small-Data/data/splits/train_50.csv\n",
      "/Users/yioha_/Desktop/Small-Data/data/splits/train_200.csv\n",
      "/Users/yioha_/Desktop/Small-Data/data/splits/train_1000.csv\n",
      "/Users/yioha_/Desktop/Small-Data/data/splits/train_5000.csv\n",
      "/Users/yioha_/Desktop/Small-Data/reports/subset_stats.md\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "# Jay路径\n",
    "TRAIN_POOL_PATH = \"/Users/yioha_/Desktop/Small-Data/data/splits/train_pool.csv\"\n",
    "OUT_SPLITS_DIR  = \"/Users/yioha_/Desktop/Small-Data/data/splits\"\n",
    "REPORTS_DIR     = \"/Users/yioha_/Desktop/Small-Data/reports\"\n",
    "\n",
    "# Iris路径\n",
    "# TRAIN_POOL_PATH = \"/Users/iriswu/Desktop/3001 Small Data/Small-Data/data/splits/train_pool.csv\"\n",
    "# OUT_SPLITS_DIR  = \"/Users/iriswu/Desktop/3001 Small Data/Small-Data/data/splits\"\n",
    "# REPORTS_DIR     = \"/Users/iriswu/Desktop/3001 Small Data/Small-Data/reports\"\n",
    "\n",
    "# Haydee路径\n",
    "# TRAIN_POOL_PATH = \"/Users/yinghanding/Desktop/Small-Data/data/splits/train_pool.csv\"\n",
    "# OUT_SPLITS_DIR  = \"/Users/yinghanding/Desktop/Small-Data/data/splits\"\n",
    "# REPORTS_DIR     = \"/Users/yinghanding/Desktop/Small-Data/reports\"\n",
    "# ===========================\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "import json, math\n",
    "import pandas as pd\n",
    "\n",
    "# 固定参数\n",
    "RANDOM_STATE = 2025\n",
    "SIZES = [50, 200, 1000, 5000]\n",
    "REQUIRED_COLS = {\"query\", \"tools\", \"gold_call\"}\n",
    "TOOL_KEYS = (\"tool_name\", \"name\")  # gold_call 里解析工具名优先顺序\n",
    "\n",
    "# 路径对象\n",
    "TRAIN_POOL_PATH = Path(TRAIN_POOL_PATH)\n",
    "OUT_SPLITS_DIR  = Path(OUT_SPLITS_DIR)\n",
    "REPORTS_DIR     = Path(REPORTS_DIR)\n",
    "REPORT_PATH     = REPORTS_DIR / \"subset_stats.md\"\n",
    "\n",
    "def ensure_dirs(*dirs: Path) -> None:\n",
    "    for d in dirs:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_train_pool(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"找不到训练池文件：{path}\")\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    missing = REQUIRED_COLS - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"train_pool.csv 缺少必要列: {missing}\")\n",
    "    if \"_row_id\" not in df.columns:\n",
    "        df.insert(0, \"_row_id\", range(len(df)))\n",
    "    return df\n",
    "\n",
    "def parse_tool_name(gold_call_str: str) -> str:\n",
    "    if not isinstance(gold_call_str, str) or not gold_call_str.strip():\n",
    "        return \"__UNKNOWN__\"\n",
    "    try:\n",
    "        obj = json.loads(gold_call_str)\n",
    "        if isinstance(obj, dict):\n",
    "            for k in TOOL_KEYS:\n",
    "                if k in obj and isinstance(obj[k], str):\n",
    "                    return obj[k]\n",
    "        if isinstance(obj, list) and obj:\n",
    "            first = obj[0]\n",
    "            if isinstance(first, dict):\n",
    "                for k in TOOL_KEYS:\n",
    "                    if k in first and isinstance(first[k], str):\n",
    "                        return first[k]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"__UNKNOWN__\"\n",
    "\n",
    "def add_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"_label_tool\"] = [parse_tool_name(x) for x in df[\"gold_call\"].tolist()]\n",
    "    return df\n",
    "\n",
    "def proportional_allocate(counts: Dict[str, int], k: int) -> Dict[str, int]:\n",
    "    \"\"\"按总体分布比例为每个工具分配样本数，总和为 k，且不超过各类可用量。\"\"\"\n",
    "    total = sum(counts.values())\n",
    "    k = min(k, total)\n",
    "    if k <= 0:\n",
    "        return {c: 0 for c in counts}\n",
    "\n",
    "    raw = {c: k * counts[c] / total for c in counts}\n",
    "    base = {c: int(math.floor(v)) for c, v in raw.items()}\n",
    "    rem = k - sum(base.values())\n",
    "\n",
    "    # 小数部分从大到小分配余数\n",
    "    frac_sorted = sorted(((raw[c] - base[c], c) for c in counts), reverse=True)\n",
    "    for i in range(rem):\n",
    "        base[frac_sorted[i % len(frac_sorted)][1]] += 1\n",
    "\n",
    "    # 不超过各类可用\n",
    "    base = {c: min(base[c], counts[c]) for c in counts}\n",
    "\n",
    "    # 若仍不足，按剩余容量分配\n",
    "    deficit = k - sum(base.values())\n",
    "    if deficit > 0:\n",
    "        room = {c: counts[c] - base[c] for c in counts}\n",
    "        order = sorted(room.items(), key=lambda x: (-x[1], x[0]))\n",
    "        idx = 0\n",
    "        while deficit > 0 and any(v > 0 for v in room.values()):\n",
    "            c = order[idx % len(order)][0]\n",
    "            if room[c] > 0:\n",
    "                base[c] += 1\n",
    "                room[c] -= 1\n",
    "                deficit -= 1\n",
    "            idx += 1\n",
    "\n",
    "    assert sum(base.values()) == k\n",
    "    return base\n",
    "\n",
    "def make_nested_subsets(df_labeled: pd.DataFrame, sizes: List[int], seed: int) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    生成嵌套分层子集：\n",
    "      1) 每个类别生成稳定乱序的索引队列（random_state=seed+offset）\n",
    "      2) 对每个目标 size，取到该 size 为止的“配额”数量\n",
    "      3) 组合各类前缀 → 得到嵌套子集（50 ⊂ 200 ⊂ 1000 ⊂ 5000）\n",
    "    \"\"\"\n",
    "    label_col = \"_label_tool\"\n",
    "    counts = Counter(df_labeled[label_col])\n",
    "    classes = sorted(counts.keys())\n",
    "\n",
    "    # 为每个类别生成稳定乱序队列（可复现）\n",
    "    per_class_queue: Dict[str, List[int]] = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        idx = df_labeled.index[df_labeled[label_col] == cls]\n",
    "        per_class_queue[cls] = idx.to_series().sample(frac=1.0, random_state=seed + i).tolist()\n",
    "\n",
    "    taken_per_class = {c: 0 for c in classes}\n",
    "    subsets: Dict[int, pd.DataFrame] = {}\n",
    "    total_n = len(df_labeled)\n",
    "\n",
    "    for target in sorted(sizes):\n",
    "        target = min(target, total_n)\n",
    "        alloc_total = proportional_allocate(counts, target)\n",
    "        # 本轮新增：推进每类已取数量到 alloc_total\n",
    "        for c in classes:\n",
    "            need = max(0, alloc_total[c] - taken_per_class[c])\n",
    "            if need > 0:\n",
    "                taken_per_class[c] += need\n",
    "\n",
    "        # 当前子集 = 每类已取前缀并集\n",
    "        cur_idx = []\n",
    "        for c in classes:\n",
    "            cur_idx.extend(per_class_queue[c][:taken_per_class[c]])\n",
    "        cur_idx = sorted(set(cur_idx))\n",
    "\n",
    "        # 仅输出三列\n",
    "        subsets[target] = df_labeled.loc[cur_idx, [\"query\", \"tools\", \"gold_call\"]].copy()\n",
    "\n",
    "    return subsets\n",
    "\n",
    "def write_subset_stats(pool_labeled: pd.DataFrame,\n",
    "                       subsets: Dict[int, pd.DataFrame],\n",
    "                       report_path: Path) -> None:\n",
    "    lines: List[str] = []\n",
    "    pool_tools = set(pool_labeled[\"_label_tool\"].unique())\n",
    "\n",
    "    lines.append(\"# Subset Coverage Statistics\\n\\n\")\n",
    "    lines.append(f\"- train_pool size: **{len(pool_labeled)}**\\n\")\n",
    "    lines.append(f\"- train_pool unique tools: **{len(pool_tools)}**\\n\\n\")\n",
    "\n",
    "    for size in sorted(subsets.keys()):\n",
    "        df = subsets[size].copy()\n",
    "        df[\"_label_tool\"] = [parse_tool_name(x) for x in df[\"gold_call\"]]\n",
    "        tools = set(df[\"_label_tool\"].unique())\n",
    "        cov = len(tools) / max(1, len(pool_tools))\n",
    "        lines.append(f\"## train_{size}\\n\\n\")\n",
    "        lines.append(f\"- size: **{len(df)}**\\n\")\n",
    "        lines.append(f\"- unique tools: **{len(tools)}**\\n\")\n",
    "        lines.append(f\"- tool coverage vs train_pool: **{cov:.2%}**\\n\\n\")\n",
    "\n",
    "        cnt = df[\"_label_tool\"].value_counts().rename(\"count\").to_frame()\n",
    "        cnt.index.name = \"tool\"\n",
    "        cnt[\"share\"] = (cnt[\"count\"] / len(df)).round(6)\n",
    "        lines.append(\"| tool | count | share |\\n\")\n",
    "        lines.append(\"|---|---:|---:|\\n\")\n",
    "        for tool, row in cnt.sort_values([\"count\", \"tool\"], ascending=[False, True]).iterrows():\n",
    "            lines.append(f\"| {tool} | {int(row['count'])} | {row['share']:.6f} |\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "\n",
    "    report_path.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "def main():\n",
    "    ensure_dirs(OUT_SPLITS_DIR, REPORTS_DIR)\n",
    "    df_pool = read_train_pool(TRAIN_POOL_PATH)\n",
    "    df_pool = add_label(df_pool)\n",
    "\n",
    "    subsets = make_nested_subsets(df_pool, SIZES, RANDOM_STATE)\n",
    "\n",
    "    # 写 CSV（UTF-8，含表头）\n",
    "    for size, df in subsets.items():\n",
    "        df.to_csv(OUT_SPLITS_DIR / f\"train_{size}.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 覆盖率统计\n",
    "    write_subset_stats(df_pool, subsets, REPORT_PATH)\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "    for size in sorted(subsets.keys()):\n",
    "        print(OUT_SPLITS_DIR / f\"train_{size}.csv\")\n",
    "    print(REPORT_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
